{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141aa3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Author: voicebeer\n",
    "Date: 2020-09-14\n",
    "LastEditDate: 2025-04-20\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import notebook as tqdm\n",
    "from torch import nn, optim, utils\n",
    "from torch.nn import functional as F\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    colab_cwd = \"/content\"  # Default Colab working directory\n",
    "    repo_name = \"DS-AGC-Colab\"\n",
    "    repo_url = f\"https://github.com/theJingqiZhou/{repo_name}.git\"\n",
    "\n",
    "    print(f\"Google Colab detected. Cloning {repo_name} repository...\")\n",
    "\n",
    "    # Clone the repo and move necessary files to the working directory\n",
    "    os.system(\n",
    "        f\"\"\"\n",
    "        git clone {repo_url} &&\n",
    "        mv {colab_cwd}/{repo_name}/modules {colab_cwd}/ &&\n",
    "        mv {colab_cwd}/{repo_name}/datasets.py {colab_cwd}/ &&\n",
    "        rm -rf {colab_cwd}/{repo_name}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    print(f\"Setup complete. Required modules and datasets.py are now in {colab_cwd}\")\n",
    "\n",
    "import datasets, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 20\n",
    "\n",
    "DATASET_NAME = \"SEED\"\n",
    "\n",
    "DATA_DIR = os.path.join(\"data\", DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed: int = RANDOM_SEED) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def weight_init(m: nn.Module) -> None:\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        setup_seed()\n",
    "        nn.init.xavier_uniform_(\n",
    "            m.weight.data\n",
    "        )  # 对参数进行xavier初始化，为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.3)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        setup_seed()\n",
    "        m.weight.data.normal_(0, 0.03)\n",
    "        # nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_cos_similarity_distance(\n",
    "    pseudo: torch.Tensor, pred: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Get distance in cosine similarity\n",
    "    :param features: features of samples, (batch_size, num_clusters)\n",
    "    :return: distance matrix between features, (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    pseudo_norm = torch.norm(pseudo, dim=1, keepdim=True)\n",
    "    pseudo = pseudo / pseudo_norm\n",
    "\n",
    "    pred_norm = torch.norm(pred, dim=1, keepdim=True)\n",
    "    pred = pred / pred_norm\n",
    "\n",
    "    cos_dist_matrix = torch.mm(pseudo, pred.transpose(0, 1))\n",
    "    return cos_dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0949a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gcn_contrast(\n",
    "    model: modules.SemiGCL, target_loader: utils.data.DataLoader, device: str\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    assert isinstance(target_loader.dataset, utils.data.TensorDataset)\n",
    "    data_target = target_loader.dataset.tensors[0].to(device)\n",
    "    labels_target = target_loader.dataset.tensors[1].to(device)\n",
    "    pred = model.predict(data_target)\n",
    "    target_scores = pred.detach().argmax(dim=1)\n",
    "    target_acc = (\n",
    "        (target_scores == labels_target.argmax(dim=1)).float().sum().item()\n",
    "    ) / len(data_target)\n",
    "    print(\"target_acc:\", target_acc)\n",
    "    return target_acc\n",
    "\n",
    "\n",
    "def train_gcn_contrast(\n",
    "    subject_id: int,\n",
    "    parameter: dict[str, Any],\n",
    "    net_params: dict[str, Any],\n",
    "    source_labeled_loaders: utils.data.DataLoader,\n",
    "    source_unlabeled_loaders: utils.data.DataLoader,\n",
    "    target_loader: utils.data.DataLoader,\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    device = net_params[\"DEVICE\"]\n",
    "    setup_seed()\n",
    "    model = modules.SemiGCL(net_params).to(device)\n",
    "    setup_seed()\n",
    "    model.apply(weight_init)\n",
    "    awl = modules.AutomaticWeightedLoss(4)\n",
    "    optimizer = optim.RMSprop(\n",
    "        [\n",
    "            {\n",
    "                \"params\": model.parameters(),\n",
    "                \"lr\": parameter[\"init_lr\"],\n",
    "                \"weight_decay\": parameter[\"weight_decay\"],\n",
    "            },\n",
    "            {\"params\": awl.parameters(), \"weight_decay\": 0},\n",
    "        ]\n",
    "    )\n",
    "    best_acc, best_test_acc = 0.0, 0.0\n",
    "    acc_list = np.zeros(parameter[\"epochs\"])\n",
    "    threshold = parameter[\"threshold\"]\n",
    "    for epoch in range(parameter[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss, total_num, target_bar = 0.0, 0, tqdm.tqdm(target_loader)\n",
    "        source_acc_total, target_acc_total = 0, 0\n",
    "        train_source_iter_labeled = enumerate(source_labeled_loaders)\n",
    "        train_source_iter_unlabeled = enumerate(source_unlabeled_loaders)\n",
    "        setup_seed()\n",
    "        for data_target, label_target in target_bar:\n",
    "            _, (data_source, labels_source) = next(train_source_iter_labeled)\n",
    "            _, (x_un, _) = next(train_source_iter_unlabeled)\n",
    "            x_un = x_un.to(device)\n",
    "            data_source, labels_source = data_source.to(device), labels_source.to(\n",
    "                device\n",
    "            )\n",
    "            data_target, labels_target = data_target.to(device), label_target.to(device)\n",
    "            if parameter[\"T_DANN\"]:\n",
    "                tripleada = 0\n",
    "            else:\n",
    "                tripleada = 1\n",
    "            if epoch >= threshold:\n",
    "                pred, domain_loss, ajloss, contrastive_loss, sim_weight, L2 = model(\n",
    "                    torch.cat((data_source, x_un, data_target)),\n",
    "                    tripleada=tripleada,\n",
    "                    threshold=1,\n",
    "                )\n",
    "            else:\n",
    "                pred, domain_loss, ajloss, contrastive_loss, sim_weight, L2 = model(\n",
    "                    torch.cat((data_source, data_target)),\n",
    "                    tripleada=0,\n",
    "                    threshold=0,\n",
    "                )\n",
    "\n",
    "            source_pred = pred[0 : len(data_source), :]\n",
    "            target_pred = pred[-len(data_source) :, :]\n",
    "            if epoch >= threshold:\n",
    "                log_prob = F.log_softmax(sim_weight * source_pred, dim=1)\n",
    "                # log_prob = F.log_softmax(source_pred, dim=1)\n",
    "            else:\n",
    "                log_prob = F.log_softmax(source_pred, dim=1)\n",
    "\n",
    "            celoss = -torch.sum(log_prob * labels_source) / len(labels_source)\n",
    "            loss = (\n",
    "                celoss\n",
    "                + parameter[\"DANN\"] * domain_loss\n",
    "                + parameter[\"dynamic_adj\"] * ajloss\n",
    "                + parameter[\"GCL\"] * contrastive_loss\n",
    "            )\n",
    "\n",
    "            source_scores = source_pred.detach().argmax(dim=1)\n",
    "            source_acc = (\n",
    "                (source_scores == labels_source.argmax(dim=1)).float().sum().item()\n",
    "            )\n",
    "            source_acc_total += source_acc\n",
    "            target_scores = target_pred.detach().argmax(dim=1)\n",
    "            target_acc = (\n",
    "                (target_scores == labels_target.argmax(dim=1)).float().sum().item()\n",
    "            )\n",
    "            target_acc_total += target_acc\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_num += parameter[\"batch_size\"]\n",
    "            total_loss += loss.item() * parameter[\"batch_size\"]\n",
    "            epoch_train_loss = total_loss / total_num\n",
    "            target_bar.set_description(\n",
    "                f\"sub:{subject_id} \"\n",
    "                f'Train Epoch: [{epoch + 1}/{parameter[\"epochs\"]}] '\n",
    "                f\"Loss: {epoch_train_loss:.4f} \"\n",
    "                f\"source_acc:{source_acc_total / total_num * 100:.2f}% \"\n",
    "                f\"target_acc:{target_acc_total / total_num * 100:.2f}%\"\n",
    "            )\n",
    "        target_test_acc = test_gcn_contrast(model, target_loader, device)\n",
    "        acc_list[epoch] = target_test_acc\n",
    "        if best_acc < (target_acc_total / total_num):\n",
    "            best_acc = target_acc_total / total_num\n",
    "\n",
    "        if best_test_acc < target_test_acc:\n",
    "            best_test_acc = target_test_acc\n",
    "            save_dir = os.path.join(DATA_DIR, \"model_result\")\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(\n",
    "                save_dir,\n",
    "                f\"model_semi_session_{DATASET_NAME}_batch48_{parameter['threshold']}\"\n",
    "                f\"epoch_{parameter['num_of_U']}U_sub{subject_id}.pkl\"\n",
    "            )\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    print(\"best_acc:\", best_acc, \"best_test_acc:\", best_test_acc)\n",
    "    return acc_list, best_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df41a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_subject(\n",
    "    target_set: dict[str, np.ndarray],\n",
    "    source_set_labeled: dict[str, np.ndarray],\n",
    "    source_set_unlabeled: dict[str, np.ndarray],\n",
    "    subject_id: int,\n",
    "    parameter: dict[str, int],\n",
    "    net_params: dict[str, Any],\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    setup_seed()\n",
    "    torch_dataset_test = utils.data.TensorDataset(\n",
    "        torch.from_numpy(target_set[\"feature\"]),\n",
    "        torch.from_numpy(target_set[\"label\"]),\n",
    "    )\n",
    "    torch_dataset_source_labeled = utils.data.TensorDataset(\n",
    "        torch.from_numpy(source_set_labeled[\"feature\"]),\n",
    "        torch.from_numpy(source_set_labeled[\"label\"]),\n",
    "    )\n",
    "    torch_dataset_source_unlabeled = utils.data.TensorDataset(\n",
    "        torch.from_numpy(source_set_unlabeled[\"feature\"]),\n",
    "        torch.from_numpy(source_set_unlabeled[\"label\"]),\n",
    "    )\n",
    "\n",
    "    source_labeled_loaders = utils.data.DataLoader(\n",
    "        dataset=torch_dataset_source_labeled,\n",
    "        batch_size=parameter[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    source_unlabeled_loaders = utils.data.DataLoader(\n",
    "        dataset=torch_dataset_source_unlabeled,\n",
    "        batch_size=parameter[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    target_loader = utils.data.DataLoader(\n",
    "        dataset=torch_dataset_test,\n",
    "        batch_size=parameter[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    acc = train_gcn_contrast(\n",
    "        subject_id,\n",
    "        parameter,\n",
    "        net_params,\n",
    "        source_labeled_loaders,\n",
    "        source_unlabeled_loaders,\n",
    "        target_loader,\n",
    "    )\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(parameter: dict[str, int], net_params: dict[str, Any]) -> list[float]:\n",
    "    # data preparation\n",
    "    setup_seed()\n",
    "    match net_params[\"category_number\"]:\n",
    "        case 3:\n",
    "            print(\"Model name: MS-MDAER. Dataset name: SEED\")\n",
    "            sub_num = 15\n",
    "        case 4:\n",
    "            print(\"Model name: MS-MDAER. Dataset name: SEED_IV\")\n",
    "            sub_num = 15\n",
    "        case 5:\n",
    "            print(\"Model name: MS-MDAER. Dataset name: SEED_V\")\n",
    "            sub_num = 16\n",
    "        case _:\n",
    "            pass\n",
    "\n",
    "    print(f'BS: {parameter[\"batch_size\"]}, epoch: {parameter[\"epochs\"]}')\n",
    "    # store the results\n",
    "    csub: list[float] = []\n",
    "    # for session_id_main in range(3):\n",
    "    session_id = 0\n",
    "    # subject_id = 0\n",
    "\n",
    "    best_acc_mat = np.zeros(sub_num)\n",
    "    target_acc_curve = np.zeros((sub_num, parameter[\"epochs\"]))\n",
    "\n",
    "    for subject_id in range(sub_num):\n",
    "        match net_params[\"category_number\"]:\n",
    "            case 3:\n",
    "                target_set, source_set_labeled, source_set_unlabeled = (\n",
    "                    datasets.seed.load_dataset(\n",
    "                        DATA_DIR, subject_id, session_id, parameter\n",
    "                    )\n",
    "                )\n",
    "                DATASET_NAME = \"SEED\"\n",
    "            case 4:\n",
    "                target_set, source_set_labeled, source_set_unlabeled = (\n",
    "                    datasets.seed_iv.load_dataset(DATA_DIR, subject_id, parameter)\n",
    "                )\n",
    "                DATASET_NAME = \"SEEDIV\"\n",
    "            case 5:\n",
    "                target_set, source_set_labeled, source_set_unlabeled = (\n",
    "                    datasets.seed_v.load_dataset(DATA_DIR, subject_id, parameter)\n",
    "                )\n",
    "                DATASET_NAME = \"SEEDV\"\n",
    "            case _:\n",
    "                pass\n",
    "\n",
    "        acc = cross_subject(\n",
    "            target_set,\n",
    "            source_set_labeled,\n",
    "            source_set_unlabeled,\n",
    "            subject_id,\n",
    "            parameter,\n",
    "            net_params,\n",
    "        )\n",
    "        csub.append(acc[1])\n",
    "        target_acc_curve[subject_id, :] = acc[0]\n",
    "        best_acc_mat[subject_id] = acc[1]\n",
    "    print(\"Cross-subject: \", csub)\n",
    "\n",
    "    result_list = {\n",
    "        \"best_acc_mat\": best_acc_mat,\n",
    "        \"target_acc_curve\": target_acc_curve,\n",
    "    }\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"model_result\"), exist_ok=True)\n",
    "    np.save(\n",
    "        os.path.join(DATA_DIR, \"model_result\",\n",
    "        f'result_list_semi_session_{DATASET_NAME}_batch48_{parameter[\"threshold\"]}'\n",
    "        f'epoch_{parameter[\"num_of_U\"]}U.npy'),\n",
    "        result_list,  # type: ignore[reportArgumentType]\n",
    "    )\n",
    "    return csub\n",
    "\n",
    "\n",
    "parameter = {\n",
    "    \"epochs\": 100,\n",
    "    \"init_lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"semi\": 1,\n",
    "    \"threshold\": 30,\n",
    "    \"num_of_U\": 2,\n",
    "    \"GCL\": 1,\n",
    "    \"dynamic_adj\": 1,\n",
    "    \"DANN\": 1,\n",
    "    \"T_DANN\": 1,\n",
    "    \"batch_size\": 48,\n",
    "}\n",
    "\n",
    "net_params = {\n",
    "    \"category_number\": 3,\n",
    "    \"DEVICE\": (\n",
    "        torch.accelerator.current_accelerator().type\n",
    "        if torch.accelerator.is_available()\n",
    "        else \"cpu\"\n",
    "    ),\n",
    "    \"node_feature_hidden1\": 5,\n",
    "    \"num_of_vertices\": 62,\n",
    "    \"linearsize\": 128,\n",
    "    \"drop_rate\": 0.8,\n",
    "    \"batch_size\": 48,  # Also used by class `modules.models.SemiGCL`\n",
    "    \"Multi_att\": 1,\n",
    "    \"num_of_features\": 5,\n",
    "    \"GLalpha\": 0.01,\n",
    "    \"K\": 3,\n",
    "}\n",
    "\n",
    "csub = main(parameter, net_params)\n",
    "\n",
    "c = np.load(\n",
    "    f'result_list_semi_session_{DATASET_NAME}_batch48_{parameter[\"threshold\"]}'\n",
    "    f'epoch_{parameter[\"num_of_U\"]}U.npy',\n",
    "    allow_pickle=True,\n",
    ").item()\n",
    "c_mean = np.mean(c[\"best_acc_mat\"])\n",
    "c_std = np.std(c[\"best_acc_mat\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
